## 2023.01.03

- ### Reproducibility

Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.

There are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release

```python
# 1. Set `os env`
os.environ['PYTHONHASHSEED'] = str(seed)
# 2. Set `python` built-in pseudo-random generator at a fixed value
random.seed(seed)
# 3. Set `numpy` pseudo-random generator at a fixed value
np.random.seed(seed)
# 4. Set `torch`
torch.manual_seed(seed)
# 5. Set `train_test_split`
train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(seed))
```

By the setting above, we will get identical output with the same input, for further reference:https://pytorch.org/docs/stable/notes/randomness.html



- ### Channels Last Memory Format in PyTorch

Channels last memory format is an alternative way of ordering NCHW tensors in memory preserving dimensions ordering, which is **more efficient** than normal ordering using with amp. Channels last tensors ordered in such a way that channels become the densest dimension (aka storing images pixel-per-pixel).

For example, classic (contiguous) storage of NCHW tensor (in our case it is two 4x4 images with 3 color channels) look like this: 

![classic_memory_format](https://pytorch.org/tutorials/_images/classic_memory_format.png)



Channels last memory format orders data differently:

![channels_last_memory_format](https://pytorch.org/tutorials/_images/channels_last_memory_format.png)



Pytorch supports memory formats (and provides back compatibility with existing models including eager, JIT, and TorchScript) by utilizing existing strides structure. For example, 10x3x16x16 batch in Channels last format will have strides equal to (768, 1, 48, 3).

<u>Stride</u> is the jump necessary to go from one element to the next one in the specified dimension [`dim`](https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim).  

> For BCWH == 10x3x16x16 (contiguous), stride == 3\*16\*16==768 when dim == 0, and stride == 16\*16 == 256 when dim == 1, stride == (768, 256, 16, 1) if dim is not specified.
>
> For BCWH == 10x3x16x16 (channel last, but the shapes are identical), stride == 16\*16\*3==768 when dim == 0, and stride == 16\*3 == 48 when dim == 1, stride == ~~(768, 48, 1, 3)~~ (768, 1, 48, 3) if dim is not specified. As the figure shown above, the interval between channel is 1 in channel last format.

**The tensor shape will not be changed when converted to Channel_Last format, only the memory format will be altered to get a higher efficiency.**

Channels last memory format is implemented for 4D NCHW Tensors only. Consequently, in MSFusion model, we need to convert tensors to Channel_Last before fusion operation(Branch, Batch, Channel, W, H)